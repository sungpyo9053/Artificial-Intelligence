# -*- coding: utf-8 -*-
"""artificialintelligence.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lELoGyNMcfUdvOQkuw8KYVm_VGvDxKBQ
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
# %matplotlib inline
import matplotlib
import matplotlib.pyplot as plt
import numpy.random as rnd
import os



x = 2*np.random.rand(100,1)
y = 4+3*x +np.random.randn(100,1)
plt.plot(x,y,"b")
plt.xlabel("$x_1$",fontsize = 18)
plt.ylabel("$y$",rotation = 0 , fontsize=18)
plt.axis([0,2,0,15])
plt.show()

x_b = np.c_[np.ones((100,1)),x]
theta_best = np.linalg.inv(x_b.T.dot(x_b)).dot(x_b.T).dot(y)
print (theta_best)

x_new = np.array([[0],[2]])
x_new_b = np.c_[np.ones((2,1)), x_new]
y_predict = x_new_b.dot(theta_best)
print (y_predict)

plt.plot(x,y,"b")
plt.xlabel("$x_1$",fontsize = 18)
plt.ylabel("$y$", rotation = 0 , fontsize = 14)
plt.axis([0,2,0,15])
plt.show()

from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()
lin_reg.fit(x,y)

print(lin_reg.intercept_)
print(lin_reg.coef_)

theta_best_svd, residuals , rank , s = np.linalg.lstsq(x_b, y, rcond = 1e-6)
print (theta_best_svd)

eta = 0.1
n_iterations = 1000
m =100
theta = np.random.randn(2,1)
for iteration in range(n_iterations):
  gradients = 2/m * x_b.T.dot(x_b.dot(theta)-y)
  theta = theta - eta * gradients
print(theta)
print(x_new_b.dot(theta))

theta_path_bgd = []
def plot_gradient_descent(theta, eta, theta_path = None):
  m = len(x_b)
  plt.plot(x,y,"b")
  n_iterations = 1000
  for iteration in range (n_iterations):
    if iteration <10:
      y_predict = x_new_b.dot(theta)
      style = "b-" if iteration > 0 else "r--"
      plt.plot(x_new,y_predict,style)
    gradients = 2/m * x_b.T.dot(x_b.dot(theta) - y)
    theta = theta - eta * gradients
    if theta_path is not None:
      theta_path.append(theta)
  plt.xlabel("$x_1$", fontsize=18)
  plt.axis([0,2,0,15])
  plt.title(r"$eta = {}$".format(eta),fontsize=16)

np.random.seed(42)
theta = np.random.randn(2,1)
plt.figure(figsize = (10,4))
plt.subplot(131); plot_gradient_descent(theta,eta = 0.02)
plt.ylabel("$y$",rotation = 0, fontsize=18)
plt.subplot(132); plot_gradient_descent(theta, eta = 0.1, theta_path=theta_path_bgd)
plt.subplot(133); plot_gradient_descent(theta, eta = 0.5)
plt.show()

theta_path_sgd = []
m = len(x_b)
np.random.seed(42)
n_epochs = 50
t0,t1 = 5, 50

def learning_schedule(t):
  return t0/(t+t1)
theta = np.random.randn(2,1)

for epoch in range(n_epochs):
  for i in range(m):
    if epoch == 0 and i < 20:
      y_predict = x_new_b.dot(theta)
      style = "b-" if i>0 else "r--"
      plt.plot(x_new,y_predict,style)
    random_index = np.random.randint(m)
    xi = x_b[random_index:random_index+1]
    yi = y[random_index:random_index+1]
    gradients = 2*xi.T.dot(xi.dot(theta)- yi)
    eta = learning_schedule(epoch*m+i)
    theta = theta -eta *gradients
    theta_path_sgd.append(theta)
  
plt.plot(x,y,"b")
plt.xlabel("$y$",fontsize=18)
plt.ylabel("$y$", rotation = 0, fontsize=18)
plt.axis([0,2,0,15])
plt.show()
print(theta)



from sklearn.linear_model import SGDRegressor
sgd_reg = SGDRegressor(max_iter=50, penalty =None,eta0=0.1,random_state=42)

temp = sgd_reg.fit(x,y.ravel())
print(temp)
print(sgd_reg.intercept_)
print(sgd_reg.coef_)

theta_path_mgd = []
n_iterations = 50
minibatch_size = 20
np.random.seed(42)
theta = np.random.randn(2,1)
t0,t1 = 200, 1000
def learning_schedule(t):
  return t0/(t+t1)
t = 0
for epoch in range(n_iterations):
  shuffled_indices = np.random.permutation(m)
  x_b_shuffled = x_b[shuffled_indices]
  y_shuffled = y[shuffled_indices]
  for i in range(0,m,minibatch_size):
    t +=1
    xi=x_b_shuffled[i:i+minibatch_size]
    yi=y_shuffled[i:i+minibatch_size]
    gradients=2/minibatch_size*xi.T.dot(xi.dot(theta)-yi)
    eta = learning_schedule(t)
    theta=theta-eta*gradients
    theta_path_mgd.append(theta)
    
print(theta)

theta_path_bgd = np.array(theta_path_bgd)
theta_path_sgd = np.array(theta_path_sgd)
theta_path_mgd = np.array(theta_path_mgd)

plt.figure(figsize=(7,4))
plt.plot(theta_path_sgd[:,0],theta_path_sgd[:, 1],"r-s",linewidth=1,label="SGD")
plt.plot(theta_path_mgd[:,0],theta_path_mgd[:, 1],"g-+",linewidth=2,label="MINI_BATCH")
plt.plot(theta_path_bgd[:,0],theta_path_bgd[:, 1],"b-o",linewidth=3,label="BATCH")
plt.legend(loc="upper left",fontsize=16)
plt.xlabel(r"$\theta_0$",fontsize=20)
plt.ylabel(r"$\theta_1$     ", fontsize=20, rotation=0)
plt.axis([2.5,4.5,2.3,3.9])
plt.show()

np.random.seed(42)
m = 100
x = 6*np.random.rand(m,1) -3
y = 0.5 * x**2 + x+ 2 + np.random.randn(m,1)
plt.plot(x,y,"b.")
plt.xlabel("$x_1$",fontsize=18)
plt.ylabel("$y$", rotation=0, fontsize=18)
plt.axis([-3,3,0,10])
plt.show()

from sklearn.preprocessing import PolynomialFeatures
poly_features = PolynomialFeatures(degree=2, include_bias = False)
x_poly = poly_features.fit_transform(x)

print(x[0])
print(x_poly[0])

lin_reg=LinearRegression()
lin_reg.fit(x_poly,y)
print(lin_reg.intercept_)
print(lin_reg.coef_)

x_new=np.linspace(-3,3,100).reshape(100,1)
x_new_poly=poly_features.transform(x_new)
y_new=lin_reg.predict(x_new_poly)
plt.plot(x,y,"b")
plt.plot(x_new,y_new,"r-",linewidth=2,label="prediction")
plt.xlabel("$x_1$",fontsize=18)
plt.ylabel("$y$",rotation=0,fontsize=18)
plt.legend(loc="upper left",fontsize=14)
plt.axis([-3,3,0,10])
plt.show()

from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

for style,width,degree in (("g-",1,300),("b--",2,2),("r-+",2,1)):
  polybig_features = PolynomialFeatures(degree=degree,include_bias=False)
  std_scaler = StandardScaler()
  lin_reg = LinearRegression()
  polynomial_regression = Pipeline([
      ("poly_features",polybig_features),
      ("std_scaler",std_scaler),
      ("ine_reg",lin_reg),
  ])
  polynomial_regression.fit(x,y)
  y_newbig = polynomial_regression.predict(x_new)
  plt.plot(x_new,y_newbig,style,label=str(degree),linewidth=width)
plt.plot(x,y,"b",linewidth=3)
plt.legend(loc="upper left")
plt.xlabel("$x_1$",fontsize = 18)
plt.ylabel("$y$",rotation = 0, fontsize = 18)
plt.axis([-3,3,0,10])
plt.show()

from sklearn.linear_model import Ridge

np.random.seed(42)
m = 20
x = 3*np.random.rand(m,1)
y = 1 + 0.5*x + np.random.randn(m,1)/1.5
x_new = np.linspace(0,3,100).reshape(100,1)

def plot_model(model_class, polynomial,alphas, **model_kargs):
  for alpha, style in zip(alphas,("b-","g--","r:")):
    model = model_class(alpha,**model_kargs) if alpha > 0 else LinearRegression()
    if polynomial:
      model = Pipeline([
          ("poly_features", PolynomialFeatures(degree=10,include_bias=False)),
          ("std_scaler",StandardScaler()),
          ("regul_reg",model),
      ])
    model.fit(x,y)
    y_new_regul = model.predict(x_new)
    lw = 2 if alpha > 0 else 1
    plt.plot(x_new,y_new_regul,style,linewidth=lw,label=r"$\alpha = {}$".format(alpha))
  plt.plot(x,y,"b.",linewidth=3)
  plt.legend(loc="upper left", fontsize=15)
  plt.xlabel("$x_1$",fontsize=18)
  plt.axis([0,3,0,4])
plt.figure(figsize=(8,4))
plt.subplot(121)
plot_model(Ridge,polynomial=False,alphas=(0,10,100),random_state=42)
plt.ylabel("$y$",rotation = 0, fontsize=18)
plt.subplot(122)
plot_model(Ridge,polynomial=True, alphas=(0,10**-5,1),random_state=42)
plt.show()

def logit(z):
  return 1 / (1+ np.exp(-z))

def relu(z):
  return np.maximum(0,z)

def derivative(f,z,eps=0.000001):
  return (f(z + eps) - f(z- eps))/(2*eps)

z = np.linspace(-5,5,200)

plt.figure(figsize=(11,4))

plt.subplot(121)
plt.plot(z,np.sign(z),"r-",linewidth=2,label="step")
plt.plot(z, logit(z), "g--", linewidth=2, label="sigmoid")
plt.plot(z, np.tanh(z),"b--",linewidth=2,label="tanh")
plt.plot(z, relu(z), "m-.",linewidth=2, label ="ReLU")
plt.grid(True)
plt.legend(loc="center right", fontsize=14)
plt.title("activation function: g(z)", fontsize=14)
plt.axis([-5,5,-1.2,1.2])

plt.subplot(122)
plt.plot(z,derivative(np.sign,z),"r-",linewidth=2,label="step")
plt.plot(0,0,"ro",markersize=5)
plt.plot(0,0,"rx",markersize=10)
plt.plot(z,derivative(logit,z),"g--",linewidth=2,label="sigmoid")
plt.plot(z,derivative(np.tanh,z),"b--",linewidth=2,label="tanh")
plt.plot(z,derivative(relu,z),"m--",linewidth=2,label="ReLU")
plt.grid(True)
plt.title("gradient: g(z)",fontsize=14)
plt.axis([-5,5,-0.2,1.2])

plt.show()

np.random.seed(0)
N,D = 3,4

x = np.random.randn(N,D)
y = np.random.randn(N,D)
z = np.random.randn(N,D)

a = x * y
b = a + z
c = np.sum(b)

grad_c = 1.0
grad_b = grad_c * np.ones((N,D))
grad_a = grad_b.copy()
grad_z = grad_b.copy()
grad_x = grad_a*y
grad_y = grad_a*x

print(grad_c)
print(grad_b)
print(grad_a)
print(grad_z)
print(grad_x)
print(grad_y)

import torch

x = torch.randn(N,D,requires_grad=True)
y = torch.randn(N,D,requires_grad=True)
z = torch.randn(N,D)

a = x * y
b = a + z
c = torch.sum(b)

c.backward()


print (x)
print (y)

N,D_in,H,D_out = 64,1000,100,10

x = torch.randn(N,D_in)
y = torch.randn(N,D_out)
w1 = torch.randn(D_in,H,requires_grad=True)
w2 = torch.randn(H,D_out,requires_grad=True)

learning_rate = 1e-6

for t in range(500):
  y_pred = x.mm(w1).clamp(min=0).mm(w2)
  loss = (y_pred - y).pow(2).sum()

  print(t,loss.item())
 
  loss.backward()  
  
  with torch.no_grad():
    w1 -=learning_rate *w1.grad
    w2 -=learning_rate *w2.grad
    w1.grad.zero_()
    w2.grad.zero_()

